
step 0: train loss 10.9873, val loss 10.9915
iter 0: loss 11.0254, time 22493.31ms, mfu -100.00%
Traceback (most recent call last):
  File "train.py", line 393, in <module>
    scaler.step(optimizer)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 285, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/yxdong/nano-gpt-test/nanoGPT-eva/eva.py", line 255, in step
    self._precondition_grads()
  File "/home/yxdong/nano-gpt-test/nanoGPT-eva/eva.py", line 150, in _precondition_grads
    grad = self._get_grad(module)
  File "/home/yxdong/nano-gpt-test/nanoGPT-eva/eva.py", line 230, in _get_grad
    grad = self.beta1 * self.old_grad + (1 - self.beta1) * grad
TypeError: unsupported operand type(s) for *: 'float' and 'builtin_function_or_method'