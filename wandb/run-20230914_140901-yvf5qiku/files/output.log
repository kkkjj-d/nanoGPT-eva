
step 0: train loss 10.9873, val loss 10.9915
Traceback (most recent call last):
  File "train.py", line 393, in <module>
    scaler.step(optimizer)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 285, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/yxdong/nano-gpt-test/nanoGPT-eva/eva.py", line 251, in step
    self._precondition_grads()
  File "/home/yxdong/nano-gpt-test/nanoGPT-eva/eva.py", line 207, in _precondition_grads
    nu = torch.nn.utils.clip_grad_norm_((self.kl_clip / vg_sum),1.0)
  File "/home/yxdong/anaconda3/envs/dyx-py38/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py", line 32, in clip_grad_norm_
    parameters = [p for p in parameters if p.grad is not None]
TypeError: 'float' object is not iterable